# 02.0 - YugabyteDB Smart Driver

To get started we used the vanilla Postgres JDBC driver.  This is fine for basic use, but we will eventually want to
use the [YugabyteDB Smart Driver](https://docs.yugabyte.com/preview/drivers-orms/java/yugabyte-jdbc/).  This is actually
a wrapper around the Postgres JDBC driver that provides additional configuration options including load balancing and
topology awareness.

To add the driver, update the `build.gradle` file and add (don't replace the postgres driver):

```gradle
    runtimeOnly 'com.yugabyte:jdbc-yugabytedb:42.3.5-yb-3'
```

To configure this, update the datasource URL and add the driver class (this is important to 
disambiguate from the Postgres driver):

```yaml
    url: jdbc:yugabytedb://localhost:5433/yugabyte
    driver-class-name: com.yugabyte.Driver
```

Restart the application to confirm these changes work correctly.

# 02.1 - Enable Load Balancing

Load balancing is a feature of the YugabyteDB driver that can evenly distribute connections to multiple available nodes.
The driver will attempt to pick nodes that have fewer connections.  This will help to more evenly distribute load across
a multiple node cluster without needing an additional infrastructure load balancer in the middle.

To enable load balancing, simply add this to the datasource url:

```yaml
  url: jdbc:yugabytedb://localhost:5433/yugabyte?load-balance=true
```

But how do we know it is working?

First, let's enable some additional logging for the driver in the `application.yml`:

```yaml
logging.level:
  com.yugabyte.Driver: DEBUG
```

If you restart the application you'll see additional messages like this:

```
2023-05-09T16:16:14.441-04:00 DEBUG 387025 --- [onnection adder] com.yugabyte.Driver                      : ClusterAwareLoadBalancer: updating connection count for 172.17.0.2 by 1
```

In fact, you should see this for every connection in the HikariCP connection pool.  However, they look like they are
all going to the same instance.  It doesn't look like load balancing is working.

That is because we are only running a single node Docker instance.

Revert the logging level back to INFO:

```yaml
logging.level:
  com.yugabyte.Driver: INFO
```

# 02.2 - Deploy a Multiple Node YugabyteDB Cluster

Stop the Docker process:

```shell
$ docker ps
$ docker stop <container_id>
```

Docker Compose provides another option.

With compose, we can even configure the YugabyteDB process to include additional configuration options that mimic a
production cluster more closely.  In the `infra/docker/` are two `docker-compose` files.  These can be used to emulate a
single node or 3 node cluster respectively.  Depending on your systems available resources, you may need to restrict
yourself to the single node cluster.

```shell
$ cd infra/docker
$ docker compose up -d
```
^ If you have the standalone docker compose, you may need to use the `docker-compose` command instead of the plugin.

When the container(s) have started, exec into the first one and test out the `ysqlsh` interface:

```shell
$ docker compose exec -it yb-node1 bash

[root@yb-node1 yugabyte]# ysqlsh -h `hostname -i`
ysqlsh (11.2-YB-2.16.3.0-b0)
Type "help" for help.

yugabyte=# \q
```

Assuming the instance(s) starts correctly:

Restart the application and verify that the connections are spread out (if
you are limited to a single-node instance, these will be limited to the one node):

```
2023-05-09T16:50:20.569-04:00 DEBUG 404099 --- [onnection adder] com.yugabyte.Driver                      : ClusterAwareLoadBalancer: Host chosen for new connection: 172.22.0.2
...
2023-05-09T16:50:20.594-04:00 DEBUG 404099 --- [onnection adder] com.yugabyte.Driver                      : ClusterAwareLoadBalancer: Host chosen for new connection: 172.22.0.4
...
2023-05-09T16:50:20.622-04:00 DEBUG 404099 --- [onnection adder] com.yugabyte.Driver                      : ClusterAwareLoadBalancer: Host chosen for new connection: 172.22.0.3
```

This configuration of the load balancer spreads the load across all available nodes in the cluster.  In some
configurations this may not be desirable (.e. multi-region) and additional "topology" configuration can be specified
with an additional parameter:

```yaml
  url: jdbc:yugabytedb:...&topology-keys=gcp.us-east1.*:1,gcp.us-central1.*:2,gcp.us-west1.*:3
```

## What are YugabyteDB Topology Keys? 

Topology keys are used to identify a YugabyteDB node deployment location by `<cloud>`, `<region>`, and `<zone>`.  When
using the driver to load balance, these keys can be used to filter the nodes that it will be allowed to connect to.
More than one key may be provided separated by a comma.  To specify all zones in a region, use an expression
`cloud.region.*`.

To designate fallback regions/zones if the primary is unreachable, specify a priority in the form `:n`, where
`n` is the order of precedence. For example, `cloud1.datacenter1.rack1:1,cloud1.datacenter1.rack2:2`.

## Summary

The application is running and is establishing connections to the YugabyteDB container(s).  This takes care of the
initial "plumbing" required.  Next, lets us Flyway to create and manage the database schema and testing data.
